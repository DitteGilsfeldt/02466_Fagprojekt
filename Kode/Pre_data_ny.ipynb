{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_bool(size):\n",
    "    return np.random.rand(size) > 0.5\n",
    "\n",
    "def random_int(min, max, size):\n",
    "    return np.random.randint(min, max, size=size)\n",
    "\n",
    "def random_float(min, max, size):\n",
    "    return np.random.uniform(min, max, size=size)\n",
    "\n",
    "def index_id(max):\n",
    "    list = []\n",
    "    for i in range(1, max):\n",
    "        list.append('ID' + str(i))\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_op = pd.DataFrame({'IDno': index_id(501), 'Død inden for 1 år af operation': random_bool(500), 'age': random_int(18, 89, 500), 'sex': random_bool(500), 'Respiratory Disease': random_bool(500),\n",
    "       'Circulatory Organ Disease': random_bool(500), 'Type 1 Diabetes': random_bool(500), 'Type 2 Diabetes': random_bool(500),\n",
    "       'Other metabolic diseases': random_bool(500), 'Other operation': random_bool(500), 'Genital or urine related diseases': random_bool(500), 'Vægt': random_float(35, 190,500), 'KFN': random_bool(500), 'KFX': random_bool(500), 'KFK': random_bool(500), 'KFM': random_bool(500),\n",
    "       'KFF': random_bool(500), 'KFP': random_bool(500), 'KFC': random_bool(500), 'KFW': random_bool(500), 'KFJ': random_bool(500), 'Hæmoglobin': random_float(4,12,500), 'Leukocytter': random_float(1, 125, 500),\n",
    "       'Trombocytter': random_float(20, 1000, 500)})\n",
    "pre_op = pre_op.replace(True, 1).replace(False, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Mortality with pre_op data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#TODO other params?\n",
    "def lr_pred(X_train, y_train, X_test):\n",
    "    lr = LogisticRegression()\n",
    "    lr_grid = {'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "            'C': [100, 10, 1.0, 0.1, 0.01],\n",
    "                'max_iter': [200,300]}\n",
    "    \n",
    "    grid_result = GridSearchCV(estimator = lr, param_grid = lr_grid, scoring='precision', cv=5)\n",
    "    grid_result.fit(X_train,y_train)\n",
    "    \n",
    "    best_model = grid_result.best_estimator_ \n",
    "\n",
    "    lr_preds = best_model.predict_proba(X_test)[:,1]\n",
    "    return lr_preds\n",
    "\n",
    "    #TODO Take Best result of the fit or can i just return the predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rf_pred(X_train, y_train, X_test):\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    rf_grid = {'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2,5,10,15,22],\n",
    "    'n_estimators': [100, 200, 300, 1000]}\n",
    "    \n",
    "    grid_result = GridSearchCV(estimator = rf, param_grid = rf_grid, scoring='precision', cv=5)\n",
    "    grid_result.fit(X_train,y_train)\n",
    "    \n",
    "    best_model = grid_result.best_estimator_ \n",
    "\n",
    "    rf_preds = best_model.predict_proba(X_test)[:,1]\n",
    "    return rf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the target and features\n",
    "target = 'Død inden for 1 år af operation'\n",
    "features = [col for col in pre_op.columns if col != target and col != 'IDno']\n",
    "\n",
    "X = pre_op[features]\n",
    "y = pre_op[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, balanced=False):\n",
    "        self.balanced = balanced\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self, random_state):\n",
    "        params = {\n",
    "            \"max_depth\": 2,\n",
    "            \"learning_rate\": 1,\n",
    "            \"n_estimators\": 10,\n",
    "            \"subsample\": 0.5,\n",
    "            \"random_state\": random_state.get_state()[1][0],\n",
    "        }\n",
    "        if self.balanced:\n",
    "            params[\"scale_pos_weight\"] = \"balanced\"\n",
    "        self.model = GradientBoostingClassifier(**params)\n",
    "        return self.model\n",
    "\n",
    "    def fit_model(self, model, train_data, train_labels):\n",
    "        model.fit(train_data, train_labels)\n",
    "\n",
    "    def predict_model(self, model, test_data):\n",
    "        preds = model.predict_proba(test_data)[:, 1]\n",
    "        return preds\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model, path):\n",
    "        import joblib\n",
    "        joblib.dump(model, path)\n",
    "\n",
    "# Initialize the model\n",
    "gb_model = GradientBoosting()\n",
    "\n",
    "# Create the model parameters\n",
    "model = gb_model.create_model(np.random.RandomState(42))\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit_model(model, X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gb_model.predict_model(model, X_test)\n",
    "\n",
    "# Convert probabilities to binary outcomes\n",
    "threshold = 0.5\n",
    "binary_predictions = (predictions >= threshold).astype(int)\n",
    "\n",
    "# Convert the probabilities to a numpy array and print it\n",
    "probabilities_array = np.array(predictions)\n",
    "print(probabilities_array)\n",
    "\n",
    "# Evaluate the binary predictions\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "conf_matrix = confusion_matrix(y_test, binary_predictions)\n",
    "class_report = classification_report(y_test, binary_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = pre_op.loc[:, pre_op.columns != 'Død inden for 1 år af operation']\n",
    "X = X.loc[:, X.columns != 'IDno']\n",
    "y = pre_op['Død inden for 1 år af operation']\n",
    "\n",
    "outer_cv = GroupKFold(n_splits = 5)\n",
    "inner_cv = GroupKFold(n_splits = 5)\n",
    "\n",
    "#TODO add gbm and mlp into this dictionary\n",
    "results = {\n",
    "    'lr': {'pr_auc': [], 'roc_auc': [], 'precision': [], 'recall': [], 'fpr': [], 'tpr': []},\n",
    "    'rf': {'pr_auc': [], 'roc_auc': [], 'precision': [], 'recall': [], 'fpr': [], 'tpr': []}\n",
    "}\n",
    "\n",
    "def evaluate_model(pred_func, model_name):\n",
    "    for outer_train_idx, outer_test_idx in outer_cv.split(X, y, groups = pre_op['IDno']):\n",
    "        X_outer_train, y_outer_train = X.iloc[outer_train_idx], y.iloc[outer_train_idx]\n",
    "        X_outer_test, y_outer_test = X.iloc[outer_test_idx], y.iloc[outer_test_idx]\n",
    "\n",
    "        inner_cv = GroupKFold(n_splits=5)\n",
    "        inner_pr_auc_scores = []\n",
    "        inner_roc_auc_scores = []\n",
    "\n",
    "        for inner_train_idx, inner_test_idx in inner_cv.split(X_outer_train, y_outer_train, groups=pre_op['IDno'].iloc[outer_train_idx]):\n",
    "            X_inner_train, y_inner_train = X_outer_train.iloc[inner_train_idx], y_outer_train.iloc[inner_train_idx]\n",
    "            X_inner_test, y_inner_test = X_outer_train.iloc[inner_test_idx], y_outer_train.iloc[inner_test_idx]\n",
    "            #TODO Make the perfomance class that returns PR AUC, ROC AUC and PR and ROC\n",
    "\n",
    "            y_pred_prob = pred_func(X_inner_train, y_inner_train, X_inner_test)\n",
    "    \n",
    "            precision, recall, _ = precision_recall_curve(y_inner_test, y_pred_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            inner_pr_auc_scores.append(pr_auc)\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(y_inner_test, y_pred_prob)\n",
    "            roc_auc = roc_auc_score(y_inner_test, y_pred_prob)\n",
    "            inner_roc_auc_scores.append(roc_auc)\n",
    "\n",
    "        y_pred_prob = pred_func(X_outer_train, y_outer_train, X_outer_test)\n",
    "\n",
    "        # Compute PR AUC\n",
    "        precision, recall, _ = precision_recall_curve(y_outer_test, y_pred_prob)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        results[model_name]['pr_auc'].append(pr_auc)\n",
    "\n",
    "        # Compute ROC AUC\n",
    "        fpr, tpr, _ = roc_curve(y_outer_test, y_pred_prob)\n",
    "        roc_auc = roc_auc_score(y_outer_test, y_pred_prob)\n",
    "        results[model_name]['roc_auc'].append(roc_auc)\n",
    "\n",
    "        results[model_name]['precision'].append(precision)\n",
    "        results[model_name]['recall'].append(recall)\n",
    "        results[model_name]['fpr'].append(fpr)\n",
    "        results[model_name]['tpr'].append(tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC AUC and PR AUC calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lr_pred, 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_pred, 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model_name):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot PR curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(len(results[model_name]['precision'])):\n",
    "        plt.plot(results[model_name]['recall'][i], results[model_name]['precision'][i], alpha=1, label=f'Outer fold {i+1}', linewidth=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'PR curve for {model_name.upper()}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(len(results[model_name]['fpr'])):\n",
    "        plt.plot(results[model_name]['fpr'][i], results[model_name]['tpr'][i], alpha=1, label=f'Outer fold {i+1}', linewidth=2)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC curve for {model_name.upper()}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results('lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_auc_values(model_name):\n",
    "    print(f\"Model: {model_name.upper()}\")\n",
    "    for i in range(len(results[model_name]['roc_auc'])):\n",
    "        print(f\"Fold {i+1} - ROC AUC: {results[model_name]['roc_auc'][i]:.4f}, PR AUC: {results[model_name]['pr_auc'][i]:.4f}\")\n",
    "    print(f\"Average ROC AUC: {np.mean(results[model_name]['roc_auc']):.4f}\")\n",
    "    print(f\"Average PR AUC: {np.mean(results[model_name]['pr_auc']):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_auc_values('lr')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
